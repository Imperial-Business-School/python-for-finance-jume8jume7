{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel Data\n",
    "\n",
    "Sometimes, data comes in such a way that many observations share certain common features. For example, several measurements can be made in the same location, under the same condition, or for the same subject. To understand the data and extract meaningful insights, we often need to aggregate these observations. This is where the groupby() function comes into play."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading\n",
    "\n",
    "As always, let's start by importing pandas and loading and cleaning our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"data/sp500_q1_2025.csv\")\n",
    "\n",
    "# Look at dates - back in the ISO format!\n",
    "print(\"Look at our dates:\\n\", df.DlyCalDt.head())\n",
    "\n",
    "# Convert the 'datadate' column to a datetime object\n",
    "df.DlyCalDt = pd.to_datetime(df.DlyCalDt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll stop short of setting the index as our datetime value though. This is because an index must have unique values, and because this panel data contains lots of different company stocks for just one quarter of a year, we'll see the same date lots of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique dates exist in the data frame\n",
    "print(\"Number of unique dates\", df.DlyCalDt.nunique())\n",
    "\n",
    "# Last date in the dataset\n",
    "print(\"Last date\", df.DlyCalDt.max())\n",
    "\n",
    "# First date in the dataset\n",
    "print(\"First date\", df.DlyCalDt.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "Let's not forget data cleaning! Do we have missing data? Where?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing data\", df.isnull().sum().sum())\n",
    "\n",
    "# Find missing data\n",
    "df[df.isnull().any(axis=1)]\n",
    "\n",
    "# Given the amount of missing data, it may be best to drop these rows\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(\"Missing data after cleaning\", df.isnull().sum().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring\n",
    "\n",
    "Let's explore this panel data a bit more, to answer some questions:\n",
    "\n",
    "- How many tickers are considered\n",
    "- How many securities are considered\n",
    "- How many companies are considered\n",
    "- Which exchanges are considered\n",
    "- Which exchanges appear most\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUnique tickers:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mdf\u001b[49m.Ticker.nunique())\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUnique companies\u001b[39m\u001b[33m\"\u001b[39m, df.PERMCO.nunique())\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUnique securities\u001b[39m\u001b[33m\"\u001b[39m, df.PERMNO.nunique())\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Unique tickers:\", df.Ticker.nunique())\n",
    "print(\"Unique companies\", df.PERMCO.nunique())\n",
    "print(\"Unique securities\", df.PERMNO.nunique())\n",
    "# Notice the discrepancy between these values - we'll look more at why this is when we learn to group\n",
    "\n",
    "# If we use unique() instead of nunique() we'll get the actual values\n",
    "print(\"Unique exhanges:\", df.PrimaryExch.unique())\n",
    "print(\"Exchanges by appearance:\", df.PrimaryExch.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping\n",
    "\n",
    "What if we wanted to calculate daily returns in this data set. Is it as simple as using `pct_change()`? Let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Returns\"] = df.DlyClose.pct_change()\n",
    "df # switch to 25 per page and go to page 3 (row index 60) to see the problem!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see what's gone wrong here? Our first calculated daily return for American Airlines is using Agilent's last closing price. This hopefully gets across the importance of *grouping*, particularly useful with this kind of panel data.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can solve this with the `groupby()` method of data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Returns\"] = df.groupby(\"PERMNO\").DlyClose.pct_change()\n",
    "df.head(65) # just navigate to the last page of the dataframe view for row index 60"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Grouping is a very powerful way to manipulate panel data. Once you've grouped, you can call functions and they will be applied groupwise as we saw above. Here are some other common functions with groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the number of rows in each group\n",
    "print(\"Number of rows per group\", df.groupby(\"Ticker\").size())\n",
    "\n",
    "# Subset a specific group\n",
    "apple = df.groupby(\"Ticker\").get_group(\"AAPL\")\n",
    "apple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what else we can do with grouping. Recall that we had more tickers than companies. Let's see why that is by looking at how many unique tickers belong to each company (using `Ticker` and `PERMCO`). Then let's list those companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a series with the number of unique stocks for each company\n",
    "ticker_counts = df.groupby(\"PERMCO\").Ticker.nunique()\n",
    "multi_permco = ticker_counts[ticker_counts > 1].index\n",
    "\n",
    "# Then we index that series with a boolean expression\n",
    "df[df.PERMCO.isin(multi_permco)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Tick Tick\n",
    "\n",
    "**Part 1** Identify the number of unique tickers traded on each exchange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2** Then identify any securities that share a ticker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE GOES HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation\n",
    "\n",
    "Aggregation functions like `mean()`, `median()`, `sum()`, `min()`, `max()`, `first()`, `last()` and `std()` can be applied to grouped data to give insights across panel data. Say we wanted the average daily return of each traded security, or the max volume traded on any given day for each security?\n",
    "\n",
    "The exercises above helped us identify that the `PERMNO` column corresponds to unique securities, so let's use that for grouping from now on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"PERMNO\").Returns.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful, but only to a point. The `PERMNO` value is just a number to most of us. What if we want a ticker or name for the security? Let's look at grouping by multiple columns to help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"PERMNO\", \"Ticker\"], as_index=False).DlyClose.min() # as_index is optional. It keeps our columns as columns, allowing pretty display of a dataframe\n",
    "\n",
    "df.groupby([\"PERMNO\", \"SecurityNm\"], as_index=False).DlyClose.first() # notice this is different to min(), and the first price of the period\n",
    "\n",
    "df.groupby([\"PERMNO\", \"Ticker\"], as_index=False).SecurityNm.first() # first() is commonly used for aggregating like, non-numeric data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've done these sorts of aggregation, we're often curious to see who sits at the top or the bottom of the distribution. We can use `nlargest()` and its antonym here. Note that `as_index=False` doesn't work here easily, since these functions refer to the index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"SecurityNm\").Returns.mean().nsmallest()\n",
    "\n",
    "df.groupby(\"SecurityNm\").DlyVol.max().nlargest()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also group by multiple columns! This can be helpful when doing aggregation, for example, to find high performers in each month. Because our date is just a regular column, we need to specify `.dt` to use any datetime functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a column to specify the month\n",
    "df[\"Month\"] = df.DlyCalDt.dt.month_name()\n",
    "\n",
    "# Then use it to group and aggregate for max closing price each month\n",
    "df.groupby([\"Ticker\", \"Month\"], as_index=False).DlyClose.max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Good Days\n",
    "\n",
    "Which two days of the week see the highest average close in this data set, and what is the average close for those days?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE GOES HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Trading Exchanges\n",
    "\n",
    "Next identify the total trading volume of each exchange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: The 1000 Club\n",
    "\n",
    "For securities that reached a closing price above 1000, how many times in each month, did they acheive this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE GOES HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Aggregation\n",
    "\n",
    "We can use the `agg()` method, and pass it a dictionary to do multiple aggregations at once on grouped data. This can be helpful for further analyses, or for producing a more descriptive aggregated data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"PERMNO\").agg({\"SecurityNm\": \"first\", \"DlyClose\": \"mean\"})\n",
    "\n",
    "df.groupby(\"PERMNO\").agg({\"SecurityNm\": \"first\", \"DlyClose\": [\"first\", \"last\"], \"DlyVol\": \"sum\"}).nlargest(5, columns=(\"DlyVol\", \"sum\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Quarter Query\n",
    "\n",
    "Using multiple aggregation, create an aggregated data frame with ticker and security name, the first open price in the period for each security and the last close price in the period for each security. Create a new column in this aggregated data frame that shows the price difference between final close and initial open for each security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE GOES HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "Now that we've got clean data, let's start with some basic financial analysis.\n",
    "\n",
    "First, let's load our CSV file into a DataFrame, covert our dates, set the index, and check for duplicated rows or missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data \"13/08/2020\" doesn't match format \"%m/%d/%Y\", at position 1. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m df = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33mTSLA_10_clean.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m df = df.set_index(\u001b[33m\"\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m\"\u001b[39m).sort_index().drop_duplicates()\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDuplicates:\u001b[39m\u001b[33m\"\u001b[39m, df.duplicated().sum())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:1067\u001b[39m, in \u001b[36mto_datetime\u001b[39m\u001b[34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[39m\n\u001b[32m   1065\u001b[39m         result = arg.map(cache_array)\n\u001b[32m   1066\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1067\u001b[39m         values = \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1068\u001b[39m         result = arg._constructor(values, index=arg.index, name=arg.name)\n\u001b[32m   1069\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc.MutableMapping)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:433\u001b[39m, in \u001b[36m_convert_listlike_datetimes\u001b[39m\u001b[34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33mmixed\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    435\u001b[39m result, tz_parsed = objects_to_datetime64(\n\u001b[32m    436\u001b[39m     arg,\n\u001b[32m    437\u001b[39m     dayfirst=dayfirst,\n\u001b[32m   (...)\u001b[39m\u001b[32m    441\u001b[39m     allow_object=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    442\u001b[39m )\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    445\u001b[39m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[32m    446\u001b[39m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:467\u001b[39m, in \u001b[36m_array_strptime_with_fallback\u001b[39m\u001b[34m(arg, name, utc, fmt, exact, errors)\u001b[39m\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_array_strptime_with_fallback\u001b[39m(\n\u001b[32m    457\u001b[39m     arg,\n\u001b[32m    458\u001b[39m     name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    462\u001b[39m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    463\u001b[39m ) -> Index:\n\u001b[32m    464\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[33;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[32m    466\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m     result, tz_out = \u001b[43marray_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    468\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tz_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    469\u001b[39m         unit = np.datetime_data(result.dtype)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mstrptime.pyx:501\u001b[39m, in \u001b[36mpandas._libs.tslibs.strptime.array_strptime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mstrptime.pyx:451\u001b[39m, in \u001b[36mpandas._libs.tslibs.strptime.array_strptime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mstrptime.pyx:583\u001b[39m, in \u001b[36mpandas._libs.tslibs.strptime._parse_with_format\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: time data \"13/08/2020\" doesn't match format \"%m/%d/%Y\", at position 1. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"TSLA_10_clean.csv\")\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df = df.set_index(\"Date\").sort_index().drop_duplicates()\n",
    "\n",
    "print(\"Duplicates:\", df.duplicated().sum())\n",
    "print(\"Missing:\", df.isnull().sum().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returns\n",
    "\n",
    "Returns refer to the gain or loss made on an initial investment, often expressed as a percentage. We can use the generic **percentage change** formula here:\n",
    "\n",
    "$$ (price_{end} - price_{start}) / price_{start} $$\n",
    "\n",
    "We can apply this to close prices to calculate the simple daily return:\n",
    "\n",
    "$$ (close price_{today} - close price_{yesterday}) / close price_{yesterday} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m jan31_closing = \u001b[43mdf\u001b[49m.loc[\u001b[33m\"\u001b[39m\u001b[33m2024-01-31\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mClose\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      2\u001b[39m jan30_closing = df.loc[\u001b[33m\"\u001b[39m\u001b[33m2024-01-30\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mClose\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      4\u001b[39m jan31_return = (jan31_closing - jan30_closing) / jan30_closing\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "jan31_closing = df.loc[\"2024-01-31\", \"Close\"]\n",
    "jan30_closing = df.loc[\"2024-01-30\", \"Close\"]\n",
    "\n",
    "jan31_return = (jan31_closing - jan30_closing) / jan30_closing\n",
    "print(f\"Return on 31 Jan was {jan31_return:.2%}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple daily return expresses a loss in value of 2.24% from one day to the next. Notice we leave our return in decimal form, but when we output it we use `f-strings` and `:.2%` to display it as a percentage.\n",
    "\n",
    "If we wanted to use the above approach to calculate daily returns for each day in our data set, it would take a long time. Let's see how we can use pandas `pct_change()` to make this sort of work easy, by applying our percentage change formula one column at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the first row in our data has a missing value **NaN** in the new daily return column. This is because our data doesn't have a close price for the day before it!\n",
    "\n",
    "What to do with this missing value depends on what further analysis we want to do. If we want to carry out simple descriptive statistics like compute the mean, max, or standard deviation,  we can leave our missing value as NaN, because pandas will by default ignore NaNs when calculating these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more complex analyses though, we may want to drop or fill this value. Let's calculate cumulative returns for the period. Instead of comparing a given day with the day before it, cumulative returns compare a given day with the first day of the period, to indicate how our stock has performed since our initial investment.\n",
    "\n",
    "We generally fill missing daily returns with a 0, which indicates no change with the day before.\n",
    "\n",
    "Because we're doing cumulative multiplication, we'll add 1 to the closing price to get a growth factor, so we can compound the return over time. After calculating the cumulative product, we subtract 1 to get back to a return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Bull or Bear?\n",
    "\n",
    "Calculate the daily change in trading volume over the data frame. Then calculate the average change in trading volume over the period. Take the same approach we used for calculating daily returns, but considering `Volume` instead of `Close` price.\n",
    "\n",
    "Then determine the trend in TSLA's stock for **Q1 2024**:\n",
    "\n",
    "- Rising volume and increasing price might indicate a **bullish** trend (where the uptrend is backed by strong demand and could continue).\n",
    "\n",
    "- Rising volume and decreasing price might indicate a **bearish** trend (where the downtrend is backed by strong selling pressure and could continue).\n",
    "\n",
    "- Falling volume on price increase or decrease often indicate a trend is losing strength. It might suggest that momentum is waning and a price reversal is coming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE GOES HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surges\n",
    "\n",
    "Surges in price or trading volume can be helpful indicators for traders. One way to define a surge is as an increase on the day before by an amount higher than some defined threshold. That threshold is often defined as some number of standard deviations above the mean. Let's look at price surges of five standard deviations above the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Averages\n",
    "\n",
    "Moving averages are a different kind of indicator, one that smooths out small variations in trading data to give a better picture of the overall trend.\n",
    "\n",
    "A Simple Moving Average (SMA) is one which averages out a price over a specific period. The average is \"moving\" because when a new day is considered in the period, the oldest date is discarded.\n",
    "\n",
    "Moving averages can be *fast*, when they cover a short period, or *slow* when they consider a longer period. The longer the period, the more those small variations are smoothed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volatility\n",
    "\n",
    "Volatility looks at the degree of variance in a stock, and can be helpful for determining risk. Periods of high standard deviation indicate higher volatility and may suggest a riskier investment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
